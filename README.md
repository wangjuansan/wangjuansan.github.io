<!-- 👋 Hi! I am Juan Wang(王娟 in Chinese). I work at [DAMO Academy](https://damo.alibaba.com/) <a href='https://damo.alibaba.com/' target="_blank"><img src='./images/alibaba.png' align="center" style='vertical-align: middle; width: 78px;'></a> as an Algorithm Expert in Hangzhou. I received my Ph.D. degree from [Zhejiang University](http://www.zju.edu.cn/) <a href="http://www.zju.edu.cn/" target="_blank"><img src='./images/zju.png' align="center" style='vertical-align: middle; width: 19px;'></a> in June 2024, affiliated with a joint program with [Westlake University](https://www.westlake.edu.cn/) <a href="https://www.westlake.edu.cn/" target="_blank"><img src='./images/westlake.png' align="center" style='vertical-align: middle; width: 19px;'></a> at [Machine Intelligence Laboratory (MiLAB)](https://milab.westlake.edu.cn/) and advised by Prof. [Donglin Wang](https://en.westlake.edu.cn/faculty/donglin-wang.html). Before that, I received my B.Eng. Degree from School of Computer Science, [Wuhan University](https://www.whu.edu.cn/) <a href="https://www.whu.edu.cn/" target="_blank"><img src='./images/whu.png' align="center" style='vertical-align: middle; width: 19px;'></a> in June 2019. -->

👋 I'm a PhD student of [the Laboratory for Image Information Communications](http://www.iic.ecei.tohoku.ac.jp/) <a href="http://www.iic.ecei.tohoku.ac.jp/" target="_blank"></a> at [Tohoku University](https://www.tohoku.ac.jp/en/) <a href="https://www.tohoku.ac.jp/en/" target="_blank"></a> under the supervisor of Prof. [Shinichiro Omachi](https://scholar.google.com/citations?user=5YWMFLUAAAAJ&hl=en). Previously, I received M.S. from [HoHai University](https://en.hhu.edu.cn/) <a href="https://en.hhu.edu.cn/" target="_blank"></a> in Jun. 2021 and B.S. from JiShou University in Jun. 2018.


<!-- 🔬 My research has centered on the **3D perception, understanding, reasoning, and generation of multimodal (including images, videos, language, dynamics, etc.) data from both the internet and the physical world**. I also focus on **efficientAI** (in terms of data, time, parameters, memory, etc.) for multimodal applications. I have published 20+ papers <a class='all_citation_badges' href="" target="_blank"></a> on the above topics at the top-tier international AI conferences. Recently, I devote myself to the development of multi-modal generative, embodied, and unified foundation models. -->
<!-- <a href="https://scholar.google.com/citations?user=mhpkWSYAAAAJ" target="_blank"><img src="https://img.shields.io/badge/dynamic/json?label=Paper%20Citations&query=total_citations&url=https%3A%2F%2Fcse.bth.se%2F~fer%2Fgooglescholar-api%2Fgooglescholar.php%3Fuser%3DmhpkWSYAAAAJ&logo=googlescholar&style=social" align="center" alt="Google Scholar"></a> -->

🔬 My research interest includes **3D Scene Understanding, Object Detection, Image Recognition, Multi-modal (including images, language, 3d point cloud.)** and their applications based on deep learning and Convolutional Neural Networks.

<!-- 🌟 I am looking for job opportunities and . If you are interedted any form of **academic cooperation**, please feel free to email me at **siteng.huang**[AT]**gmail.com** (replace [AT] with @). Additionly, I maintain close cooperation with [MiLAB](https://milab.westlake.edu.cn/) from Westlake University. This top-tier robot learning lab is actively looking for **visiting students and RAs** (please refer to [Recruitment](https://milab.westlake.edu.cn/contact.html)). Specially, if you are willing to cooperate with me there, please also send me a copy when sending your CV to the lab. -->

🌟 I am currently seeking job opportunities and collaborators. If you are interested in my research, please feel free to contact me via **wang.juan.t4**[AT]**dc.tohoku.ac.jp** (replace [AT] with @).

Welcome to refer to my full publication list at [my personal homepage](https://wangjuansan.github.io/#-publications).


### 📢 News

<!-- 参考 https://huanwang.tech/ 的样式 -->

<!-- * <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/02/24</span> **[Preprint]** We released [Humanoid-VLA](https://arxiv.org/abs/2502.14795), a novel framework that integrates language understanding, egocentric scene perception, and motion control, enabling universal humanoid control!

* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2024/03/13</span> **[ICME'24]** One paper ([DARA](https://arxiv.org/abs/2405.06217)) about parameter-efficient tuning for visual grounding got accepted for ICME 2024 (Oral). -->